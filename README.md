# Local AI Voice Assistant

A fully offline AI voice assistant capable of interpreting user commands, generating responses using a small language model, converting responses to speech, and simulating hardware actions like turning on/off lights or fans.

## üåü Features

- **Offline Speech Recognition (ASR)**: Uses Whisper (small) to transcribe voice commands locally
- **Simple Language Model (SLM)**: Uses DistilGPT2 to interpret commands and generate responses
- **Text-to-Speech (TTS)**: Converts response text to audio using pyttsx3
- **Hardware Simulation**: Simulates GPIO actions for lights, fans, or other devices
- **Interactive Loop**: Supports multiple consecutive commands in one session
- **Logging**: Tracks speech, hardware actions, and errors

## üèóÔ∏è System Architecture

```
User Voice ‚Üí Microphone ‚Üí ASR (Whisper) ‚Üí Simple Language Model (DistilGPT2)
    ‚Üì                         ‚Üì                           ‚Üì
Audio Recording      Text Command Parsing        Decision / Action
    ‚Üì                         ‚Üì                           ‚Üì
TTS (pyttsx3) ‚Üê Response Text ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
Speaker Output
    ‚Üì
Optional: Hardware Controller (Simulated GPIO / Fan / Light)
```

## üìÅ Project Structure

```
voice_assistant/
‚îú‚îÄ‚îÄ assistant.py                    # Main script to run the assistant
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ audio/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ speech_to_text.py      # ASR (Whisper)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text_to_speech.py      # TTS (pyttsx3)
‚îÇ   ‚îú‚îÄ‚îÄ hardware/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hardware_controller.py # Simulated GPIO actions
‚îÇ   ‚îú‚îÄ‚îÄ ai/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ language_model.py      # DistilGPT2-based SLM
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ logger.py              # Logging for speech, hardware, errors
‚îî‚îÄ‚îÄ README.md
```

## üìã Requirements

- **Python**: 3.9+
- **Dependencies**: Listed in `requirements.txt`

```txt
torch>=2.1.0
transformers>=4.30.0
pyttsx3>=2.90
sounddevice>=0.4.6
numpy>=1.24.0
simpleaudio>=1.0.4
soundfile>=0.12.1
whisper @ git+https://github.com/openai/whisper.git
```

> **Note:** All models (Whisper & DistilGPT2) should be pre-downloaded in your HuggingFace cache directory (`C:\Users\[Username]\.cache\huggingface\hub`) for offline use.

## üöÄ Installation

### 1. Clone the Repository
```bash
git clone <your_repo_url>
cd voice_assistant
```

### 2. Create Virtual Environment
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux / Mac
source venv/bin/activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Pre-download Models for Offline Usage
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# DistilGPT2
model = AutoModelForCausalLM.from_pretrained("distilgpt2", cache_dir=r"C:\Users\[Username]\.cache\huggingface\hub")
tokenizer = AutoTokenizer.from_pretrained("distilgpt2", cache_dir=r"C:\Users\[Username]\.cache\huggingface\hub")

# Whisper (small) - will be downloaded automatically when first used
```

## üéØ Usage

### Run the Assistant
```bash
python assistant.py
```

### Voice Commands
The assistant responds to commands such as:
- "Turn on light"
- "Turn off fan"
- "Wave hello"

### Process Flow
1. **Voice Input**: Speak your command
2. **ASR Processing**: Whisper converts speech to text
3. **Language Processing**: DistilGPT2 interprets the command
4. **Response Generation**: Creates appropriate response
5. **TTS Output**: Converts response to speech
6. **Hardware Action**: Simulates device control (optional)
7. **Logging**: Records all actions

### Exit
Press `Ctrl+C` to stop the assistant

## üí¨ Example Interaction

```
üé§ Listening...
üéôÔ∏è Recording...
üë§ You: Please turn on light.
ü§ñ Assistant: Turning on the light.
‚ö° [SIMULATION] TURN ON light (Pin: PIN_17)
```

## üîß Hardware Simulation

### Current Implementation
- **Light/Fan/Servo Simulation**: Actions are printed to console and logged
- **Pin Mapping**: Simulated GPIO pin assignments
- **Action Logging**: All hardware commands are recorded

### Future GPIO Integration
The system is designed to easily integrate with real hardware:
- **Raspberry Pi GPIO**
- **Arduino via pySerial**
- **NVIDIA Jetson boards**

Use the `HardwareController` class to map commands to actual pins and actions.

## üìä Logging

Logs are automatically stored in the `logs/` directory:

- **`speech.log`** ‚Äì All transcribed text and voice interactions
- **`hardware.log`** ‚Äì All simulated/actual hardware actions
- **`error.log`** ‚Äì System errors and exceptions

## üìù Notes & Recommendations

### Performance Optimization
- Keep models cached locally for true offline operation
- Use lightweight models to minimize CPU/RAM usage
- Optimize prompt design for SLM to prevent repetitive outputs

### Quality Improvements
- **TTS Enhancement**: Consider upgrading to Coqui TTS or Silero for better voice quality
- **ASR Accuracy**: Fine-tune Whisper model for specific use cases
- **Response Quality**: Improve prompt engineering for more natural responses

### System Requirements
- **RAM**: Minimum 4GB recommended (8GB+ for better performance)
- **Storage**: ~2GB for cached models
- **Microphone**: Any USB or built-in microphone
- **Speakers**: Audio output device required

## üîÆ Future Extensions

### Hardware Integration
- [ ] Replace simulated hardware with real GPIO actions
- [ ] Integrate with smart home devices (Philips Hue, etc.)
- [ ] Support for multiple device types and protocols

### Feature Enhancements
- [ ] Improve TTS quality and voice variety
- [ ] Add wake word detection
- [ ] Implement multi-step command sequences
- [ ] Add voice training for better recognition
- [ ] Support for multiple languages

### Advanced Capabilities
- [ ] Context-aware conversations
- [ ] Learning user preferences
- [ ] Integration with calendar and reminders
- [ ] Weather and news updates (with internet connection)
